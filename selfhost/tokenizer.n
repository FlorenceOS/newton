const std = @import("std");

const source_files = @import("source_files.n");
const error = @import("error.n");

const MAX_TOKENS = 0x100000;

const TokenType = enum(u8) {
    end_of_file,

    identifier,
    int_literal,
    string_literal,

    break_keyword,
    //case_keyword,
    comptime_keyword,
    const_keyword,
    //continue_keyword,
    //else_keyword,
    //endcase_keyword,
    //enum_keyword,
    //false_keyword,
    fn_keyword,
    if_keyword,
    inline_keyword,
    loop_keyword,
    //noreturn_keyword,
    return_keyword,
    //struct_keyword,
    //switch_keyword,
    //true_keyword,
    //undefined_keyword,
    //union_keyword,
    //unreachable_keyword,
    var_keyword,
    //volatile_keyword,
    __keyword,

    //bool_keyword,
    //type_keyword,
    //void_keyword,
    //anyopaque_keyword,

    opening_curly,
    closing_curly,
    opening_paren,
    closing_paren,

    semicolon,
    dot,
    comma,
    colon,

    asterisk,
    ampersand,
    plus,
    minus,
    equals,
    assign,
    tilde,
    slash,
    percent,
    plus_plus,
    vertical_bar,
};

const TokenizationContext = struct {
    file: u32,
    line: u32,
    column: u32,
    offset: u32,
    file_data: *const u8,

    fn advance(self: *@This(), num: u32) inline void {
        self.column += num;
        self.offset += num;
        self.file_data += num;
    }

    fn peek(self: *@This(), offset: u64) inline u8 {
        return self.file_data[offset];
    }

    fn matches(self: *@This(), str: *const u8) bool {
        return std.mem.equals(u8, str, self.file_data, std.string.len(str));
    }

    fn report_error(self: *@This(), message: *const u8) inline noreturn {
        error.report(self.file, self.line, self.column, self.offset, message);
    }

    fn add_token(self: *@This(), tag: TokenType) inline void {
        token_type.append_assume_capacity(tag);
        token_file.append_assume_capacity(self.file);
        token_line.append_assume_capacity(self.line);
        token_column.append_assume_capacity(self.column);
        token_offset.append_assume_capacity(self.offset);
    }

    fn add_token_advance(self: *@This(), tag: TokenType, num: u32) void {
        self.add_token(tag);
        self.advance(num);
    }
};

fn report_error_at_token(token: u32, message: *const u8) noreturn {
    error.report(
        token_file.get(token).*,
        token_line.get(token).*,
        token_column.get(token).*,
        token_offset.get(token).*,
        message,
    );
}

const Handler = *const fn(*TokenizationContext, u8) void;

var token_type: std.containers.PinnedVector(TokenType, MAX_TOKENS) = undefined;
var token_file: std.containers.PinnedVector(u32, MAX_TOKENS) = undefined;
var token_line: std.containers.PinnedVector(u32, MAX_TOKENS) = undefined;
var token_column: std.containers.PinnedVector(u32, MAX_TOKENS) = undefined;
var token_offset: std.containers.PinnedVector(u32, MAX_TOKENS) = undefined;

// TODO add array initializers
var token_handlers: [0x100]Handler = undefined;
var digit_value: [0x100]u8 = undefined;
var is_ident_char: [0x100]bool = undefined;

fn fill_handlers(start: u64, end: u64, handler: Handler) inline void {
    var curr_h = token_handlers[start].&;
    const end_h = token_handlers[end].&;
    loop {
        if(curr_h <= end_h) {
            curr_h.* = handler;
            curr_h += 1;
        } else {
            break;
        }
    }
}

fn register_single_char_token(ch: u8, comptime tt: TokenType) inline void {
    token_handlers[ch] = fn (context: *TokenizationContext) void {
        context.add_token_advance(tt, 1);
    }.&;
}

// Not inline to reduce compile times, make inline once compiler is faster
fn init() void {
    token_type.init();
    token_file.init();
    token_line.init();
    token_column.init();
    token_offset.init();

    fill_handlers(0, 0xFF, badchar_handler.&);

    {
        var ch: u32 = 0;
        loop(ch <= 0xFF) {
            is_ident_char[ch] = false;
            ch += 1;
        }
        ch = 'a';
        loop(ch <= 'z') {
            is_ident_char[ch] = true;
            ch += 1;
        }
        ch = 'A';
        loop(ch <= 'Z') {
            is_ident_char[ch] = true;
            ch += 1;
        }
        is_ident_char['_'] = true;
    }
    const ident_h = ident_handler.&;
    fill_handlers('a', 'z', ident_h);
    fill_handlers('A', 'Z', ident_h);
    token_handlers['@'] = ident_h;

    token_handlers['_'] = fn(context: *TokenizationContext) void {
        if(is_ident_char[context.peek(1)]) {
            ident_handler(context);
        } else {
            context.add_token_advance(.__keyword, 1);
        }
    }.&;

    token_handlers['b'] = fn(context: *TokenizationContext) void {
        if(context.matches("break".&)) {
            context.add_token_advance(.break_keyword, 5);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['c'] = fn(context: *TokenizationContext) void {
        if(context.matches("comptime".&)) {
            context.add_token_advance(.comptime_keyword, 8);
        }
        else if(context.matches("const".&)) {
            context.add_token_advance(.const_keyword, 5);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['f'] = fn(context: *TokenizationContext) void {
        if(context.matches("fn".&)) {
            context.add_token_advance(.fn_keyword, 2);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['i'] = fn(context: *TokenizationContext) void {
        if(context.matches("inline".&)) {
            context.add_token_advance(.inline_keyword, 6);
        }
        if(context.matches("if".&)) {
            context.add_token_advance(.if_keyword, 2);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['l'] = fn(context: *TokenizationContext) void {
        if(context.matches("loop".&)) {
            context.add_token_advance(.loop_keyword, 4);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['r'] = fn(context: *TokenizationContext) void {
        if(context.matches("return".&)) {
            context.add_token_advance(.return_keyword, 6);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['v'] = fn(context: *TokenizationContext) void {
        if(context.matches("var".&)) {
            context.add_token_advance(.var_keyword, 3);
        }
        else {
            ident_handler(context);
        }
    }.&;

    {
        var ch: u32 = 0;
        loop(ch <= 0xFF) {
            digit_value[ch] = 0xFF;
            ch += 1;
        }
        ch = '0';
        loop(ch <= '9') {
            digit_value[ch] = @truncate(u8, ch) - '0';
            ch += 1;
        }
        ch = 'a';
        loop(ch <= 'f') {
            digit_value[ch] = @truncate(u8, ch) - 'a';
            ch += 1;
        }
        ch = 'A';
        loop(ch <= 'F') {
            digit_value[ch] = @truncate(u8, ch) - 'A';
            ch += 1;
        }
    }
    fill_handlers('0', '9', fn (context: *TokenizationContext, ch: u8) void {
        context.add_token(.int_literal);
        var base: u8 = 10;
        if(ch == '0') {
            const next = context.peek(1);
            if(next == 'x') {
                context.advance(2);
                base = 16;
            }
            else if(next == 'b') {
                context.advance(2);
                base = 2;
            }
        }
        loop(digit_value[context.peek(0)] < base) {
            context.advance(1);
        }
    }.&);

    const whitespace_handler = fn(context: *TokenizationContext) void {
        context.advance(1);
    }.&;
    token_handlers[' '] = whitespace_handler;
    token_handlers['\t'] = whitespace_handler;
    token_handlers['\r'] = whitespace_handler;

    token_handlers['\n'] = fn (context: *TokenizationContext) void {
        context.line += 1;
        context.column = 1;
        context.offset += 1;
        context.file_data += 1;
    }.&;

    register_single_char_token('{', .opening_curly);
    register_single_char_token('}', .closing_curly);
    register_single_char_token('(', .opening_paren);
    register_single_char_token(')', .closing_paren);
    register_single_char_token(';', .semicolon);
    register_single_char_token('.', .dot);
    register_single_char_token(',', .comma);
    register_single_char_token('&', .ampersand);
    register_single_char_token('*', .asterisk);
    register_single_char_token('-', .minus);
    register_single_char_token('~', .tilde);
    register_single_char_token('%', .percent);
    register_single_char_token('/', .slash);
    register_single_char_token('|', .vertical_bar);

    token_handlers['+'] = fn (context: *TokenizationContext) void {
        if(context.peek(1) == '+') {
            context.add_token_advance(.plus_plus, 2);
        }
        else {
            context.add_token_advance(.plus, 1);
        }
    }.&;

    token_handlers['='] = fn (context: *TokenizationContext) void {
        if(context.peek(1) == '=') {
            context.add_token_advance(.equals, 2);
        } else {
            context.add_token_advance(.assign, 1);
        }
    }.&;

    token_handlers['"'] = fn (context: *TokenizationContext) void {
        context.add_token_advance(.string_literal, 1);
        loop {
            if(context.peek(0) == '\\') {
                context.report_error("TODO: String escape".&);
            }
            const chr = context.peek(0);
            context.advance(1);
            if(chr == '"') {
                break;
            }
        }
    }.&;
}

fn badchar_handler(context: *TokenizationContext) void {
    context.report_error("Bad character".&);
}

fn ident_handler(context: *TokenizationContext) void {
    context.add_token_advance(.identifier, 1);
    loop {
        const c = context.peek(0);
        if(is_ident_char[c]) {
            context.advance(1);
        } else {
            break;
        }
    }
}

fn tokenize_one_token(context: *TokenizationContext) inline void {
    const p: u32 = context.peek(0);
    return token_handlers[p](context, p);
}

fn token_length(token_idx: u32) u32 {
    const old_offset = token_offset.get(token_idx).*;
    const file = token_file.get(token_idx).*;

    var context = TokenizationContext{
        .file = file,
        .line = token_line.get(token_idx).*,
        .column = token_column.get(token_idx).*,
        .offset = old_offset,
        .file_data = source_files.source_files.get(file).file_data + old_offset,
    };

    tokenize_one_token(context.&);

    return context.offset - old_offset;
}

fn tokenize_file(file: u32) void {
    var context = TokenizationContext{
        .file = file,
        .line = 1,
        .column = 1,
        .offset = 0,
        .file_data = source_files.source_files.get(file).file_data,
    };

    loop {
        if(context.peek(0) == 0) {
            context.add_token(.end_of_file);
            return;
        }
        tokenize_one_token(context.&);
    }
}
