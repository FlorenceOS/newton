const std = @import("std");

const source_files = @import("source_files.n");
const error = @import("error.n");

const MAX_TOKENS = 0x100000;

const TokenType = enum(u8) {
    end_of_file,

    // Literals
    identifier,
    int_literal,
    string_literal,

    // Keywords
    break_keyword,
    //case_keyword,
    comptime_keyword,
    const_keyword,
    continue_keyword,
    else_keyword,
    //endcase_keyword,
    //false_keyword,
    fn_keyword,
    inline_keyword,
    return_keyword,
    //true_keyword,
    undefined_keyword,
    unreachable_keyword,
    var_keyword,
    //volatile_keyword,
    __keyword,

    if_keyword,
    loop_keyword,
    //switch_keyword,
    while_keyword,

    enum_keyword,
    struct_keyword,
    union_keyword,

    bool_keyword,
    type_keyword,
    void_keyword,
    noreturn_keyword,
    anyopaque_keyword,

    u8,
    u16,
    u32,
    u64,
    i8,
    i16,
    i32,
    i64,

    // Single- or multi-character tokens
    @".",
    @"..",
    @",",
    @":",
    @";",
    @"~",

    @"{",
    @".{",
    @"}",
    @"(",
    @")",
    @"[",
    @"]",

    @"+",
    @"+=",
    @"+%=",
    @"++",
    @"++=",
    @"-",
    @"-=",
    @"-%=",
    @"*",
    @"*=",
    @"/",
    @"/=",
    @"%",
    @"%=",
    @"=",
    @"==",
    @"!",
    @"!=",
    @"<",
    @"<<",
    @"<=",
    @"<<=",
    @">",
    @">>",
    @">=",
    @">>=",
    @"&",
    @"&=",
    @"&&",
    @"|",
    @"|=",
    @"||",
    @"^",
    @"^=",
};

const TokenizationContext = struct {
    file: u32,
    line: u32,
    column: u32,
    offset: u32,
    file_data: *const u8,

    fn advance(self: *@This(), num: u32) inline void {
        self.column += num;
        self.offset += num;
        self.file_data += num;
    }

    fn peek(self: *@This(), offset: u64) inline u8 {
        return self.file_data[offset];
    }

    fn match_kw(self: *@This(), str: *const u8) bool {
        const len = std.string.len(str);
        if(!std.mem.equals(u8, str, self.file_data, len)) {
            return false;
        }
        if(is_ident_char[self.file_data[len]]) {
            return false;
        }
        return true;
    }

    fn report_error(self: *@This(), message: *const u8) inline noreturn {
        error.report(self.file, self.line, self.column, self.offset, message);
    }

    fn add_token(self: *@This(), tag: TokenType) inline void {
        token_type.append_assume_capacity(tag);
        token_file.append_assume_capacity(self.file);
        token_line.append_assume_capacity(self.line);
        token_column.append_assume_capacity(self.column);
        token_offset.append_assume_capacity(self.offset);
    }

    fn add_token_advance(self: *@This(), tag: TokenType, num: u32) void {
        self.add_token(tag);
        self.advance(num);
    }
};

fn report_error_at_token(token: u32, message: *const u8) noreturn {
    error.report(
        token_file.get(token).*,
        token_line.get(token).*,
        token_column.get(token).*,
        token_offset.get(token).*,
        message,
    );
}

const Handler = *const fn(*TokenizationContext, u8) void;

var token_type: std.containers.PinnedVector(TokenType, MAX_TOKENS) = undefined;
var token_file: std.containers.PinnedVector(u32, MAX_TOKENS) = undefined;
var token_line: std.containers.PinnedVector(u32, MAX_TOKENS) = undefined;
var token_column: std.containers.PinnedVector(u32, MAX_TOKENS) = undefined;
var token_offset: std.containers.PinnedVector(u32, MAX_TOKENS) = undefined;

// TODO add array initializers
var token_handlers: [0x100]Handler = undefined;
var digit_value: [0x100]u8 = undefined;
var is_ident_char: [0x100]bool = undefined;

fn fill_handlers(start: u64, end: u64, handler: Handler) inline void {
    var curr_h = token_handlers[start].&;
    const end_h = token_handlers[end].&;
    loop {
        if(curr_h <= end_h) {
            curr_h.* = handler;
            curr_h += 1;
        } else {
            break;
        }
    }
}

fn register_single_char_token(ch: u8, comptime tt: TokenType) inline void {
    token_handlers[ch] = fn(context: *TokenizationContext) void {
        context.add_token_advance(tt, 1);
    }.&;
}

// Not inline to reduce compile times, make inline once compiler is faster
fn init() void {
    token_type.init();
    token_file.init();
    token_line.init();
    token_column.init();
    token_offset.init();

    fill_handlers(0, 0xFF, badchar_handler.&);

    {
        var ch: u32 = 0;
        while(ch <= 0xFF) {
            is_ident_char[ch] = false;
            ch += 1;
        }
        ch = 'a';
        while(ch <= 'z') {
            is_ident_char[ch] = true;
            ch += 1;
        }
        ch = 'A';
        while(ch <= 'Z') {
            is_ident_char[ch] = true;
            ch += 1;
        }
        is_ident_char['_'] = true;
    }
    const ident_h = ident_handler.&;
    fill_handlers('a', 'z', ident_h);
    fill_handlers('A', 'Z', ident_h);
    token_handlers['@'] = ident_h;

    token_handlers['_'] = fn(context: *TokenizationContext) void {
        if(is_ident_char[context.peek(1)]) {
            ident_handler(context);
        }
        else {
            context.add_token_advance(.__keyword, 1);
        }
    }.&;

    token_handlers['a'] = fn(context: *TokenizationContext) void {
        if(context.match_kw("anyopaque".&)) {
            context.add_token_advance(.anyopaque_keyword, 9);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['b'] = fn(context: *TokenizationContext) void {
        if(context.match_kw("break".&)) {
            context.add_token_advance(.break_keyword, 5);
        }
        else if(context.match_kw("bool".&)) {
            context.add_token_advance(.bool_keyword, 4);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['c'] = fn(context: *TokenizationContext) void {
        if(context.match_kw("comptime".&)) {
            context.add_token_advance(.comptime_keyword, 8);
        }
        else if(context.match_kw("const".&)) {
            context.add_token_advance(.const_keyword, 5);
        }
        else if(context.match_kw("continue".&)) {
            context.add_token_advance(.continue_keyword, 8);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['e'] = fn(context: *TokenizationContext) void {
        if(context.match_kw("else".&)) {
            context.add_token_advance(.else_keyword, 4);
        }
        if(context.match_kw("enum".&)) {
            context.add_token_advance(.enum_keyword, 4);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['f'] = fn(context: *TokenizationContext) void {
        if(context.match_kw("fn".&)) {
            context.add_token_advance(.fn_keyword, 2);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['i'] = fn(context: *TokenizationContext) void {
        if(context.match_kw("inline".&)) {
            context.add_token_advance(.inline_keyword, 6);
        }
        else if(context.match_kw("if".&)) {
            context.add_token_advance(.if_keyword, 2);
        }
        else if(context.match_kw("i8".&)) {
            context.add_token_advance(.@"i8", 2);
        }
        else if(context.match_kw("i16".&)) {
            context.add_token_advance(.@"i16", 3);
        }
        else if(context.match_kw("i32".&)) {
            context.add_token_advance(.@"i32", 3);
        }
        else if(context.match_kw("i64".&)) {
            context.add_token_advance(.@"i64", 3);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['l'] = fn(context: *TokenizationContext) void {
        if(context.match_kw("loop".&)) {
            context.add_token_advance(.loop_keyword, 4);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['n'] = fn(context: *TokenizationContext) void {
        if(context.match_kw("noreturn".&)) {
            context.add_token_advance(.noreturn_keyword, 8);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['r'] = fn(context: *TokenizationContext) void {
        if(context.match_kw("return".&)) {
            context.add_token_advance(.return_keyword, 6);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['s'] = fn(context: *TokenizationContext) void {
        if(context.match_kw("struct".&)) {
            context.add_token_advance(.struct_keyword, 6);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['t'] = fn(context: *TokenizationContext) void {
        if(context.match_kw("type".&)) {
            context.add_token_advance(.type_keyword, 4);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['u'] = fn(context: *TokenizationContext) void {
        if(context.match_kw("undefined".&)) {
            context.add_token_advance(.undefined_keyword, 9);
        }
        else if(context.match_kw("unreachable".&)) {
            context.add_token_advance(.unreachable_keyword, 11);
        }
        else if(context.match_kw("u8".&)) {
            context.add_token_advance(.@"u8", 2);
        }
        else if(context.match_kw("u16".&)) {
            context.add_token_advance(.@"u16", 3);
        }
        else if(context.match_kw("u32".&)) {
            context.add_token_advance(.@"u32", 3);
        }
        else if(context.match_kw("u64".&)) {
            context.add_token_advance(.@"u64", 3);
        }
        else if(context.match_kw("union".&)) {
            context.add_token_advance(.union_keyword, 5);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['v'] = fn(context: *TokenizationContext) void {
        if(context.match_kw("var".&)) {
            context.add_token_advance(.var_keyword, 3);
        }
        else if(context.match_kw("void".&)) {
            context.add_token_advance(.void_keyword, 4);
        }
        else {
            ident_handler(context);
        }
    }.&;

    token_handlers['w'] = fn(context: *TokenizationContext) void {
        if(context.match_kw("while".&)) {
            context.add_token_advance(.while_keyword, 5);
        }
        else {
            ident_handler(context);
        }
    }.&;

    {
        var ch: u32 = 0;
        while(ch <= 0xFF) {
            digit_value[ch] = 0xFF;
            ch += 1;
        }
        ch = '0';
        while(ch <= '9') {
            digit_value[ch] = @truncate(u8, ch) - '0';
            ch += 1;
        }
        ch = 'a';
        while(ch <= 'f') {
            digit_value[ch] = @truncate(u8, ch) - 'a';
            ch += 1;
        }
        ch = 'A';
        while(ch <= 'F') {
            digit_value[ch] = @truncate(u8, ch) - 'A';
            ch += 1;
        }
    }

    fill_handlers('0', '9', fn(context: *TokenizationContext, ch: u8) void {
        context.add_token(.int_literal);
        var base: u8 = 10;
        if(ch == '0') {
            const next = context.peek(1);
            if(next == 'x') {
                context.advance(2);
                base = 16;
            }
            else if(next == 'b') {
                context.advance(2);
                base = 2;
            }
        }
        while(digit_value[context.peek(0)] < base) {
            context.advance(1);
        }
    }.&);

    const whitespace_handler = fn(context: *TokenizationContext) void {
        context.advance(1);
    }.&;
    token_handlers[' '] = whitespace_handler;
    token_handlers['\t'] = whitespace_handler;
    token_handlers['\r'] = whitespace_handler;

    token_handlers['\n'] = fn(context: *TokenizationContext) void {
        context.line += 1;
        context.column = 1;
        context.offset += 1;
        context.file_data += 1;
    }.&;

    register_single_char_token(',', .@",");
    register_single_char_token(':', .@":");
    register_single_char_token(';', .@";");
    register_single_char_token('~', .@"~");
    register_single_char_token('{', .@"{");
    register_single_char_token('}', .@"}");
    register_single_char_token('(', .@"(");
    register_single_char_token(')', .@")");
    register_single_char_token('[', .@"[");
    register_single_char_token(']', .@"]");

    token_handlers['.'] = fn(context: *TokenizationContext) void {
        if(context.peek(1) == '.') {
            context.add_token_advance(.@"..", 2);
        }
        else {
            context.add_token_advance(.@".", 1);
        }
    }.&;

    token_handlers['+'] = fn(context: *TokenizationContext) void {
        if(context.peek(1) == '+') {
            if(context.peek(2) == '=') {
                context.add_token_advance(.@"++=", 3);
            }
            else {
                context.add_token_advance(.@"++", 2);
            }
        }
        else if(context.peek(1) == '%' && context.peek(2) == '=') {
            context.add_token_advance(.@"+%=", 3);
        }
        else if(context.peek(1) == '=') {
            context.add_token_advance(.@"+=", 2);
        }
        else {
            context.add_token_advance(.@"+", 1);
        }
    }.&;

    token_handlers['-'] = fn(context: *TokenizationContext) void {
        if(context.peek(1) == '%' && context.peek(2) == '=') {
            context.add_token_advance(.@"-%=", 3);
        }
        else if(context.peek(1) == '=') {
            context.add_token_advance(.@"-=", 2);
        }
        else {
            context.add_token_advance(.@"-", 1);
        }
    }.&;

    token_handlers['*'] = fn(context: *TokenizationContext) void {
        if(context.peek(1) == '=') {
            context.add_token_advance(.@"*=", 2);
        }
        else {
            context.add_token_advance(.@"*", 1);
        }
    }.&;

    token_handlers['/'] = fn(context: *TokenizationContext) void {
        if(context.peek(1) == '=') {
            context.add_token_advance(.@"/=", 2);
        }
        else if(context.peek(1) == '/') {
            loop {
                const ch = context.peek(0);
                if(ch == 0 || ch == '\n') {
                    return;
                }
                context.advance(1);
            }
        }
        else {
            context.add_token_advance(.@"/", 1);
        }
    }.&;

    token_handlers['%'] = fn(context: *TokenizationContext) void {
        if(context.peek(1) == '=') {
            context.add_token_advance(.@"%=", 2);
        }
        else {
            context.add_token_advance(.@"%", 1);
        }
    }.&;

    token_handlers['='] = fn(context: *TokenizationContext) void {
        if(context.peek(1) == '=') {
            context.add_token_advance(.@"==", 2);
        }
        else {
            context.add_token_advance(.@"=", 1);
        }
    }.&;

    token_handlers['!'] = fn(context: *TokenizationContext) void {
        if(context.peek(1) == '=') {
            context.add_token_advance(.@"!=", 2);
        }
        else {
            context.add_token_advance(.@"!", 1);
        }
    }.&;

    token_handlers['<'] = fn(context: *TokenizationContext) void {
        if(context.peek(1) == '<') {
            if(context.peek(2) == '=') {
                context.add_token_advance(.@"<<=", 3);
            }
            else {
                context.add_token_advance(.@"<<", 2);
            }
        }
        else if(context.peek(1) == '=') {
            context.add_token_advance(.@"<=", 2);
        }
        else {
            context.add_token_advance(.@"<", 1);
        }
    }.&;

    token_handlers['>'] = fn(context: *TokenizationContext) void {
        if(context.peek(1) == '>') {
            if(context.peek(2) == '=') {
                context.add_token_advance(.@">>=", 3);
            }
            else {
                context.add_token_advance(.@">>", 2);
            }
        }
        else if(context.peek(1) == '=') {
            context.add_token_advance(.@">=", 2);
        }
        else {
            context.add_token_advance(.@">", 1);
        }
    }.&;

    token_handlers['&'] = fn(context: *TokenizationContext) void {
        if(context.peek(1) == '&') {
            context.add_token_advance(.@"&&", 2);
        }
        else if(context.peek(1) == '=') {
            context.add_token_advance(.@"&=", 2);
        }
        else {
            context.add_token_advance(.@"&", 1);
        }
    }.&;

    token_handlers['|'] = fn(context: *TokenizationContext) void {
        if(context.peek(1) == '|') {
            context.add_token_advance(.@"||", 2);
        }
        else if(context.peek(1) == '=') {
            context.add_token_advance(.@"|=", 2);
        }
        else {
            context.add_token_advance(.@"|", 1);
        }
    }.&;

    token_handlers['^'] = fn(context: *TokenizationContext) void {
        if(context.peek(1) == '=') {
            context.add_token_advance(.@"^=", 2);
        }
        else {
            context.add_token_advance(.@"^", 1);
        }
    }.&;

    token_handlers['"'] = fn(context: *TokenizationContext) void {
        context.add_token_advance(.string_literal, 1);
        loop {
            if(context.peek(0) == '\\') {
                context.report_error("TODO: String escape".&);
            }
            const chr = context.peek(0);
            context.advance(1);
            if(chr == '"') {
                break;
            }
        }
    }.&;
}

fn badchar_handler(context: *TokenizationContext) void {
    context.report_error("Bad character".&);
}

fn ident_handler(context: *TokenizationContext) void {
    context.add_token_advance(.identifier, 1);
    loop {
        const c = context.peek(0);
        if(is_ident_char[c]) {
            context.advance(1);
        } else {
            break;
        }
    }
}

fn tokenize_one_token(context: *TokenizationContext) inline void {
    const p: u32 = context.peek(0);
    return token_handlers[p](context, p);
}

fn token_length(token_idx: u32) u32 {
    const old_offset = token_offset.get(token_idx).*;
    const file = token_file.get(token_idx).*;

    var context = TokenizationContext{
        .file = file,
        .line = token_line.get(token_idx).*,
        .column = token_column.get(token_idx).*,
        .offset = old_offset,
        .file_data = source_files.source_files.get(file).file_data + old_offset,
    };

    tokenize_one_token(context.&);

    return context.offset - old_offset;
}

fn tokenize_file(file: u32) void {
    var context = TokenizationContext{
        .file = file,
        .line = 1,
        .column = 1,
        .offset = 0,
        .file_data = source_files.source_files.get(file).file_data,
    };

    loop {
        if(context.peek(0) == 0) {
            context.add_token(.end_of_file);
            return;
        }
        tokenize_one_token(context.&);
    }
}
